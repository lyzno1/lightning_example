{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0F9N/w6Or8K8/cV78ansQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyzno1/lightning_example/blob/main/fabric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1XWZjzujK7Z",
        "outputId": "888c77ee-79b4-40be-8d8b-c9b5f577c3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.1+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n",
            "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.4.0 torchmetrics-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel('usingtest.xlsx')\n",
        "\n",
        "print(data.head())\n",
        "for i, value in enumerate(data['label']):\n",
        "    if value != 1.0 and value != 0.0:\n",
        "        print(f\"in row {i+1}\")\n",
        "print(\"yes\")\n",
        "assert all(data['label'].isin([0.0, 1.0])), \"label contains values other than 0.0 and 1.0\"\n",
        "data = data[['内容', 'label']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygq_b0q8jPfx",
        "outputId": "d78dd1d3-4d77-4638-b4a0-9488ceb4503f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    序号       昵称 性别  省份                                                 内容  \\\n",
            "0    1  潇潇diana  女  北京  一个 妈妈 一天 心路历程 吃饭 篇 牛奶 有无 三聚氰胺 超标 会 不会 喝成 大头 面包...   \n",
            "1    2   睡不饱的任镳  男  上海   发现 现在 媒体 微博后 关注度 会 大幅度 增加 快速 传播 影响 很大 有个 缺点 不...   \n",
            "2    4    狙击手蝈蝈  男  广东  铁证如山 日军 性 暴行 受害者 两姐妹 证言 公布 救 其他人 时年 14 岁 彭 仁寿 ...   \n",
            "3    7  邵井子1314  男  其他  疫苗 事件 转基因 事件 只不过 比较 两个 造假 事件 没收 转发 键 疫苗 事件 国产 ...   \n",
            "4  181  时尚老太80后  女  其他   转基因 日前 农业部 回应 表示 转基因 谣言 已经 影响 转基因 健康 发展 实际上 科...   \n",
            "\n",
            "            认证   编写日期  label  \n",
            "0          未认证   3分钟前      1  \n",
            "1  东方汇金期货研究员任镳   3分钟前      1  \n",
            "2          未认证  13分钟前      1  \n",
            "3       头条文章作者   3分钟前      1  \n",
            "4          未认证  23分钟前      1  \n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class WeiboDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        comment = self.data.iloc[idx]['内容']\n",
        "        label = self.data.iloc[idx]['label']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            comment,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )#\n",
        "        return {\n",
        "            'comment_text': comment,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "Omw95vzpjg6w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "class Classifer(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=config.num_labels).train()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        return self.bert(input_ids, attention_mask=attention_mask, labels=labels)"
      ],
      "metadata": {
        "id": "Pr0tXvcXkPSC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(config, fabric, model, val_loader):\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "    fabric.print(f\"Validation Loss\\t{avg_val_loss}\")\n",
        "\n",
        "def train(config, fabric, model, train_loader, val_loader, optimizer, scheduler=None):\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        all_steps = len(train_loader)\n",
        "        fabric.print(f\"Epoch\\t{epoch+1}\\tTotal Steps\\t{all_steps}\")\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids, attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            fabric.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            if optimizer.param_groups[0]['lr'] != scheduler.get_last_lr()[0]:\n",
        "                fabric.print(f\"Learning Rate\\t{optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "            fabric.print(f\"Epoch[{epoch+1}/{config.num_epochs}]\\tStep\\t{i}\\tAverage Train Loss\\t{avg_train_loss}\")\n",
        "\n",
        "        if epoch % config.validate_every_n_epoch == 0:\n",
        "            validate(config, fabric, model, val_loader)\n",
        "\n",
        "def test(config, fabric, model, test_loader):\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    all_f1_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids, attention_mask, labels=labels)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = outputs.loss\n",
        "            f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='binary')\n",
        "\n",
        "            test_losses.append(loss.item())\n",
        "            all_f1_scores.append(f1)\n",
        "\n",
        "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
        "    avg_f1_score = sum(all_f1_scores) / len(all_f1_scores)\n",
        "    fabric.print(f\"Test Loss\\t{avg_test_loss}\")\n",
        "    fabric.print(f\"Test F1 Score\\t{avg_f1_score}\")\n"
      ],
      "metadata": {
        "id": "YsjJPsqCv1a3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.fabric import Fabric\n",
        "\n",
        "def main(config):\n",
        "  fabric = Fabric()\n",
        "  # fabric.launch() # if multi gpu\n",
        "  fabric.seed_everything(config.seed)\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  train_data, test_data = train_test_split(data, test_size=0.1, random_state=config.seed)\n",
        "  train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=config.seed)\n",
        "\n",
        "  train_dataset = WeiboDataset(train_data, config.tokenizer, config.max_len)\n",
        "  val_dataset = WeiboDataset(val_data, config.tokenizer, config.max_len)\n",
        "  test_dataset = WeiboDataset(test_data, config.tokenizer, config.max_len)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "  model = Classifer(config)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
        "  model, optimizer = fabric.setup(model, optimizer)\n",
        "  train_loader, val_loader, test_loader = fabric.setup_dataloaders(train_loader, val_loader, test_loader)\n",
        "\n",
        "  train(config, fabric, model, train_loader, val_loader, optimizer, scheduler)\n",
        "  test(config, fabric, model, test_loader)\n",
        ""
      ],
      "metadata": {
        "id": "BGupgSRhmTos"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "@dataclass\n",
        "class Config():\n",
        "  # seed\n",
        "  seed = 42\n",
        "\n",
        "  # bert\n",
        "  num_labels = 2\n",
        "  max_len = 256\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "\n",
        "  # loader\n",
        "  batch_size = 32\n",
        "\n",
        "  # scheduler\n",
        "  step_size = 10\n",
        "  gamma = 0.9\n",
        "\n",
        "  # optimizer\n",
        "  lr = 1e-3\n",
        "\n",
        "  # training loop\n",
        "  num_epochs = 2\n",
        "  validate_every_n_epoch = 1\n"
      ],
      "metadata": {
        "id": "u-auzOakk6FZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(config=Config())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXnFqJVar4RA",
        "outputId": "18545dd3-78bd-451f-d6b7-5e5a2432da47"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 42\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 42\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch\t1\tTotal Steps\t70\n",
            "Epoch[1/2]\tStep\t0\tAverage Train Loss\t0.7799716591835022\n",
            "Epoch[1/2]\tStep\t1\tAverage Train Loss\t0.6695522964000702\n",
            "Epoch[1/2]\tStep\t2\tAverage Train Loss\t1.2686284184455872\n",
            "Epoch[1/2]\tStep\t3\tAverage Train Loss\t1.0832367837429047\n",
            "Epoch[1/2]\tStep\t4\tAverage Train Loss\t0.976738166809082\n",
            "Epoch[1/2]\tStep\t5\tAverage Train Loss\t0.9190701643625895\n",
            "Epoch[1/2]\tStep\t6\tAverage Train Loss\t0.8630381396838597\n",
            "Epoch[1/2]\tStep\t7\tAverage Train Loss\t0.9318525269627571\n",
            "Epoch[1/2]\tStep\t8\tAverage Train Loss\t0.8766976098219553\n",
            "Epoch[1/2]\tStep\t9\tAverage Train Loss\t0.8668433934450149\n",
            "Epoch[1/2]\tStep\t10\tAverage Train Loss\t0.8489433174783533\n",
            "Epoch[1/2]\tStep\t11\tAverage Train Loss\t0.8148540457089742\n",
            "Epoch[1/2]\tStep\t12\tAverage Train Loss\t0.8003633847603431\n",
            "Epoch[1/2]\tStep\t13\tAverage Train Loss\t0.7771714500018528\n",
            "Epoch[1/2]\tStep\t14\tAverage Train Loss\t0.7594929416974385\n",
            "Epoch[1/2]\tStep\t15\tAverage Train Loss\t0.7434717677533627\n",
            "Epoch[1/2]\tStep\t16\tAverage Train Loss\t0.7362517293761758\n",
            "Epoch[1/2]\tStep\t17\tAverage Train Loss\t0.719888738460011\n",
            "Epoch[1/2]\tStep\t18\tAverage Train Loss\t0.7027921002162131\n",
            "Epoch[1/2]\tStep\t19\tAverage Train Loss\t0.6780268050730228\n",
            "Epoch[1/2]\tStep\t20\tAverage Train Loss\t0.666978229369436\n",
            "Epoch[1/2]\tStep\t21\tAverage Train Loss\t0.6508827568455176\n",
            "Epoch[1/2]\tStep\t22\tAverage Train Loss\t0.6460445400165475\n",
            "Epoch[1/2]\tStep\t23\tAverage Train Loss\t0.6442255284637213\n",
            "Epoch[1/2]\tStep\t24\tAverage Train Loss\t0.6501075941324234\n",
            "Epoch[1/2]\tStep\t25\tAverage Train Loss\t0.652538004402931\n",
            "Epoch[1/2]\tStep\t26\tAverage Train Loss\t0.6413417331598423\n",
            "Epoch[1/2]\tStep\t27\tAverage Train Loss\t0.6353427400546414\n",
            "Epoch[1/2]\tStep\t28\tAverage Train Loss\t0.6321986600242812\n",
            "Epoch[1/2]\tStep\t29\tAverage Train Loss\t0.6300683453679085\n",
            "Epoch[1/2]\tStep\t30\tAverage Train Loss\t0.6283223470372539\n",
            "Epoch[1/2]\tStep\t31\tAverage Train Loss\t0.6219126437790692\n",
            "Epoch[1/2]\tStep\t32\tAverage Train Loss\t0.6229916472326625\n",
            "Epoch[1/2]\tStep\t33\tAverage Train Loss\t0.6208663289161289\n",
            "Epoch[1/2]\tStep\t34\tAverage Train Loss\t0.6206212575946536\n",
            "Epoch[1/2]\tStep\t35\tAverage Train Loss\t0.619644634011719\n",
            "Epoch[1/2]\tStep\t36\tAverage Train Loss\t0.6143605600337725\n",
            "Epoch[1/2]\tStep\t37\tAverage Train Loss\t0.6107790638741694\n",
            "Epoch[1/2]\tStep\t38\tAverage Train Loss\t0.6100074920134667\n",
            "Epoch[1/2]\tStep\t39\tAverage Train Loss\t0.6124998684972525\n",
            "Epoch[1/2]\tStep\t40\tAverage Train Loss\t0.6118979871999927\n",
            "Epoch[1/2]\tStep\t41\tAverage Train Loss\t0.6107789288674083\n",
            "Epoch[1/2]\tStep\t42\tAverage Train Loss\t0.6122364536967388\n",
            "Epoch[1/2]\tStep\t43\tAverage Train Loss\t0.6129422387616201\n",
            "Epoch[1/2]\tStep\t44\tAverage Train Loss\t0.6123708284563488\n",
            "Epoch[1/2]\tStep\t45\tAverage Train Loss\t0.6099566710383996\n",
            "Epoch[1/2]\tStep\t46\tAverage Train Loss\t0.6112031565701708\n",
            "Epoch[1/2]\tStep\t47\tAverage Train Loss\t0.6079293467725316\n",
            "Epoch[1/2]\tStep\t48\tAverage Train Loss\t0.6045349754241048\n",
            "Epoch[1/2]\tStep\t49\tAverage Train Loss\t0.597421507537365\n",
            "Epoch[1/2]\tStep\t50\tAverage Train Loss\t0.598595482169413\n",
            "Epoch[1/2]\tStep\t51\tAverage Train Loss\t0.5967580661750757\n",
            "Epoch[1/2]\tStep\t52\tAverage Train Loss\t0.5956362258150892\n",
            "Epoch[1/2]\tStep\t53\tAverage Train Loss\t0.5999118760228157\n",
            "Epoch[1/2]\tStep\t54\tAverage Train Loss\t0.5989673665978692\n",
            "Epoch[1/2]\tStep\t55\tAverage Train Loss\t0.5987773588193315\n",
            "Epoch[1/2]\tStep\t56\tAverage Train Loss\t0.5978842249564957\n",
            "Epoch[1/2]\tStep\t57\tAverage Train Loss\t0.5979048802421011\n",
            "Epoch[1/2]\tStep\t58\tAverage Train Loss\t0.597498455037505\n",
            "Epoch[1/2]\tStep\t59\tAverage Train Loss\t0.5969807761410872\n",
            "Epoch[1/2]\tStep\t60\tAverage Train Loss\t0.5963175196628101\n",
            "Epoch[1/2]\tStep\t61\tAverage Train Loss\t0.5965800446368033\n",
            "Epoch[1/2]\tStep\t62\tAverage Train Loss\t0.5954673609563282\n",
            "Epoch[1/2]\tStep\t63\tAverage Train Loss\t0.5936618766281754\n",
            "Epoch[1/2]\tStep\t64\tAverage Train Loss\t0.5916739970445632\n",
            "Epoch[1/2]\tStep\t65\tAverage Train Loss\t0.5880082911162665\n",
            "Epoch[1/2]\tStep\t66\tAverage Train Loss\t0.5859403127609794\n",
            "Epoch[1/2]\tStep\t67\tAverage Train Loss\t0.5854193320607438\n",
            "Epoch[1/2]\tStep\t68\tAverage Train Loss\t0.5852088926063068\n",
            "Epoch[1/2]\tStep\t69\tAverage Train Loss\t0.5890334271958896\n",
            "Validation Loss\t0.5562460521856943\n",
            "Epoch\t2\tTotal Steps\t70\n",
            "Epoch[2/2]\tStep\t0\tAverage Train Loss\t0.594796895980835\n",
            "Epoch[2/2]\tStep\t1\tAverage Train Loss\t0.547931045293808\n",
            "Epoch[2/2]\tStep\t2\tAverage Train Loss\t0.5311332742373148\n",
            "Epoch[2/2]\tStep\t3\tAverage Train Loss\t0.5325368195772171\n",
            "Epoch[2/2]\tStep\t4\tAverage Train Loss\t0.559233283996582\n",
            "Epoch[2/2]\tStep\t5\tAverage Train Loss\t0.5542914966742197\n",
            "Epoch[2/2]\tStep\t6\tAverage Train Loss\t0.5520073856626239\n",
            "Epoch[2/2]\tStep\t7\tAverage Train Loss\t0.5407141111791134\n",
            "Epoch[2/2]\tStep\t8\tAverage Train Loss\t0.5411295857694414\n",
            "Epoch[2/2]\tStep\t9\tAverage Train Loss\t0.5461029082536697\n",
            "Epoch[2/2]\tStep\t10\tAverage Train Loss\t0.5464323190125552\n",
            "Epoch[2/2]\tStep\t11\tAverage Train Loss\t0.5396707554658254\n",
            "Epoch[2/2]\tStep\t12\tAverage Train Loss\t0.5401443793223455\n",
            "Epoch[2/2]\tStep\t13\tAverage Train Loss\t0.543468462569373\n",
            "Epoch[2/2]\tStep\t14\tAverage Train Loss\t0.5343367258707682\n",
            "Epoch[2/2]\tStep\t15\tAverage Train Loss\t0.5264158304780722\n",
            "Epoch[2/2]\tStep\t16\tAverage Train Loss\t0.5314717205131755\n",
            "Epoch[2/2]\tStep\t17\tAverage Train Loss\t0.5274783935811784\n",
            "Epoch[2/2]\tStep\t18\tAverage Train Loss\t0.5161245292738864\n",
            "Epoch[2/2]\tStep\t19\tAverage Train Loss\t0.522944937646389\n",
            "Epoch[2/2]\tStep\t20\tAverage Train Loss\t0.5182603242851439\n",
            "Epoch[2/2]\tStep\t21\tAverage Train Loss\t0.5259577266194604\n",
            "Epoch[2/2]\tStep\t22\tAverage Train Loss\t0.5305985665839651\n",
            "Epoch[2/2]\tStep\t23\tAverage Train Loss\t0.5379072936872641\n",
            "Epoch[2/2]\tStep\t24\tAverage Train Loss\t0.541277631521225\n",
            "Epoch[2/2]\tStep\t25\tAverage Train Loss\t0.5369420120349297\n",
            "Epoch[2/2]\tStep\t26\tAverage Train Loss\t0.5381829606162177\n",
            "Epoch[2/2]\tStep\t27\tAverage Train Loss\t0.5404311631407056\n",
            "Epoch[2/2]\tStep\t28\tAverage Train Loss\t0.5400087853957867\n",
            "Epoch[2/2]\tStep\t29\tAverage Train Loss\t0.5423132499059041\n",
            "Epoch[2/2]\tStep\t30\tAverage Train Loss\t0.5414523059321988\n",
            "Epoch[2/2]\tStep\t31\tAverage Train Loss\t0.5433580782264471\n",
            "Epoch[2/2]\tStep\t32\tAverage Train Loss\t0.5474786451368621\n",
            "Epoch[2/2]\tStep\t33\tAverage Train Loss\t0.5476319842478808\n",
            "Epoch[2/2]\tStep\t34\tAverage Train Loss\t0.543801657642637\n",
            "Epoch[2/2]\tStep\t35\tAverage Train Loss\t0.541159875690937\n",
            "Epoch[2/2]\tStep\t36\tAverage Train Loss\t0.5414068739156466\n",
            "Epoch[2/2]\tStep\t37\tAverage Train Loss\t0.5352181290325365\n",
            "Epoch[2/2]\tStep\t38\tAverage Train Loss\t0.534852969340789\n",
            "Epoch[2/2]\tStep\t39\tAverage Train Loss\t0.540160396695137\n",
            "Epoch[2/2]\tStep\t40\tAverage Train Loss\t0.5395905404556088\n",
            "Epoch[2/2]\tStep\t41\tAverage Train Loss\t0.5417974051975069\n",
            "Epoch[2/2]\tStep\t42\tAverage Train Loss\t0.545518625614255\n",
            "Epoch[2/2]\tStep\t43\tAverage Train Loss\t0.5417408516461198\n",
            "Epoch[2/2]\tStep\t44\tAverage Train Loss\t0.5395763377348582\n",
            "Epoch[2/2]\tStep\t45\tAverage Train Loss\t0.539072840757992\n",
            "Epoch[2/2]\tStep\t46\tAverage Train Loss\t0.540915403594362\n",
            "Epoch[2/2]\tStep\t47\tAverage Train Loss\t0.5384774350871643\n",
            "Epoch[2/2]\tStep\t48\tAverage Train Loss\t0.538088648294916\n",
            "Epoch[2/2]\tStep\t49\tAverage Train Loss\t0.5355052715539932\n",
            "Epoch[2/2]\tStep\t50\tAverage Train Loss\t0.5341748308901694\n",
            "Epoch[2/2]\tStep\t51\tAverage Train Loss\t0.53726349484462\n",
            "Epoch[2/2]\tStep\t52\tAverage Train Loss\t0.5376476066292457\n",
            "Epoch[2/2]\tStep\t53\tAverage Train Loss\t0.5360619696202101\n",
            "Epoch[2/2]\tStep\t54\tAverage Train Loss\t0.5361330211162567\n",
            "Epoch[2/2]\tStep\t55\tAverage Train Loss\t0.5339524990745953\n",
            "Epoch[2/2]\tStep\t56\tAverage Train Loss\t0.5355053115309331\n",
            "Epoch[2/2]\tStep\t57\tAverage Train Loss\t0.533856053290696\n",
            "Epoch[2/2]\tStep\t58\tAverage Train Loss\t0.5332995304616831\n",
            "Epoch[2/2]\tStep\t59\tAverage Train Loss\t0.5328341340025265\n",
            "Epoch[2/2]\tStep\t60\tAverage Train Loss\t0.5336446776741841\n",
            "Epoch[2/2]\tStep\t61\tAverage Train Loss\t0.5330557683783193\n",
            "Epoch[2/2]\tStep\t62\tAverage Train Loss\t0.5317907655049884\n",
            "Epoch[2/2]\tStep\t63\tAverage Train Loss\t0.53493691701442\n",
            "Epoch[2/2]\tStep\t64\tAverage Train Loss\t0.5362265421794011\n",
            "Epoch[2/2]\tStep\t65\tAverage Train Loss\t0.5376385215556982\n",
            "Epoch[2/2]\tStep\t66\tAverage Train Loss\t0.5378012541514724\n",
            "Epoch[2/2]\tStep\t67\tAverage Train Loss\t0.5379160046577454\n",
            "Epoch[2/2]\tStep\t68\tAverage Train Loss\t0.5390369676161504\n",
            "Epoch[2/2]\tStep\t69\tAverage Train Loss\t0.5368570204292025\n",
            "Validation Loss\t0.5135587900876999\n",
            "Test Loss\t0.47821567952632904\n",
            "Test F1 Score\t0.9044342768866951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZquNkWsFr_8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}